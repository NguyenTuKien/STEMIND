import os
import json
import django
from django.conf import settings
from django.core.management.base import BaseCommand
from File_sharing_platform.models import File, Category
from Social_Platform.models import Post, Comment
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from dotenv import load_dotenv

# Setup Django environment
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'The_Chalk.settings')
django.setup()

class Command(BaseCommand):
    help = 'Train RAG chatbot v·ªõi d·ªØ li·ªáu t·ª´ database'

    def add_arguments(self, parser):
        parser.add_argument(
            '--output',
            type=str,
            default='stem_embeddings.json',
            help='T√™n file JSON output'
        )
        parser.add_argument(
            '--chunk-size',
            type=int,
            default=500,
            help='K√≠ch th∆∞·ªõc chunk t·ªëi ƒëa'
        )

    def handle(self, *args, **options):
        # Load environment variables
        load_dotenv()
        
        # Ki·ªÉm tra API key
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            self.stdout.write(
                self.style.ERROR("GOOGLE_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong file .env")
            )
            return
        
        self.stdout.write(f"ƒê√£ t√¨m th·∫•y API key: {api_key[:10]}...")
        
        genai.configure(api_key=api_key)
        
        output_file = options['output']
        chunk_size = options['chunk_size']
        
        # Thu th·∫≠p d·ªØ li·ªáu t·ª´ database
        training_data = self.collect_training_data()
        
        # T·∫°o embeddings
        self.create_embeddings(training_data, output_file, chunk_size)
        
        self.stdout.write(
            self.style.SUCCESS(f'ƒê√£ train RAG chatbot th√†nh c√¥ng! File: {output_file}')
        )

    def collect_training_data(self):
        """Thu th·∫≠p d·ªØ li·ªáu t·ª´ c√°c model"""
        training_data = []
        
        # L·∫•y d·ªØ li·ªáu t·ª´ File sharing platform
        try:
            files = File.objects.all()
            self.stdout.write(f"T√¨m th·∫•y {files.count()} files")
            for file in files:
                text = f"File: {file.title}\n"
                if file.file_description:
                    text += f"M√¥ t·∫£: {file.file_description}\n"
                text += f"Danh m·ª•c: {file.category.category_name}\n"
                text += f"T√°c gi·∫£: {file.author.username}\n"
                text += f"Tr·∫°ng th√°i: {'Mi·ªÖn ph√≠' if file.file_status == 0 else 'C√≥ ph√≠'}\n"
                if file.file_price > 0:
                    text += f"Gi√°: {file.file_price}\n"
                training_data.append(text)
        except Exception as e:
            self.stdout.write(
                self.style.WARNING(f"L·ªói khi l·∫•y d·ªØ li·ªáu files: {e}")
            )
        
        # L·∫•y d·ªØ li·ªáu t·ª´ Social platform
        try:
            posts = Post.objects.all()
            self.stdout.write(f"T√¨m th·∫•y {posts.count()} posts")
            for post in posts:
                text = f"Post t·ª´ {post.author.username}: {post.content}\n"
                text += f"Th·ªùi gian: {post.created_at}\n"
                training_data.append(text)
        except Exception as e:
            self.stdout.write(
                self.style.WARNING(f"L·ªói khi l·∫•y d·ªØ li·ªáu posts: {e}")
            )
        
        # L·∫•y comments
        try:
            comments = Comment.objects.all()
            self.stdout.write(f"T√¨m th·∫•y {comments.count()} comments")
            for comment in comments:
                text = f"Comment t·ª´ {comment.user.username}: {comment.content}\n"
                text += f"Th·ªùi gian: {comment.created_at}\n"
                training_data.append(text)
        except Exception as e:
            self.stdout.write(
                self.style.WARNING(f"L·ªói khi l·∫•y d·ªØ li·ªáu comments: {e}")
            )
        
        # L·∫•y categories
        try:
            categories = Category.objects.all()
            self.stdout.write(f"T√¨m th·∫•y {categories.count()} categories")
            for category in categories:
                text = f"Danh m·ª•c: {category.category_name}\n"
                training_data.append(text)
        except Exception as e:
            self.stdout.write(
                self.style.WARNING(f"L·ªói khi l·∫•y d·ªØ li·ªáu categories: {e}")
            )
        
        # Ki·ªÉm tra n·∫øu database tr·ªëng
        if not training_data:
            self.stdout.write(
                self.style.WARNING("Database tr·ªëng, kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ train.")
            )
            return []
        
        self.stdout.write(f"T·ªïng c·ªông {len(training_data)} m·∫´u d·ªØ li·ªáu ƒë·ªÉ train")
        return training_data



    def chunk_text(self, text, max_chunk_size=500):
        """Chia text th√†nh c√°c chunk nh·ªè"""
        words = text.split()
        chunks = []
        current_chunk = []
        for word in words:
            current_chunk.append(word)
            if len(current_chunk) >= max_chunk_size:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        return chunks

    def get_gemini_embedding(self, text):
        """T·∫°o embedding cho text"""
        try:
            embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
            return embeddings.embed_query(text)
        except Exception as e:
            self.stdout.write(
                self.style.WARNING(f"L·ªói khi t·∫°o embedding: {e}")
            )
            return None

    def create_embeddings(self, training_data, output_file, chunk_size):
        """T·∫°o embeddings cho t·∫•t c·∫£ d·ªØ li·ªáu training"""
        all_chunks = []
        all_embeddings = []
        
        total_texts = len(training_data)
        processed = 0
        
        for text in training_data:
            if not text.strip():
                continue
                
            chunks = self.chunk_text(text, chunk_size)
            for chunk in chunks:
                try:
                    embedding = self.get_gemini_embedding(chunk)
                    if embedding:
                        all_chunks.append(chunk)
                        all_embeddings.append(embedding)
                        self.stdout.write(f"‚úÖ ƒê√£ t·∫°o embedding cho chunk: {chunk[:100]}...")
                    else:
                        self.stdout.write(f"‚ùå Kh√¥ng th·ªÉ t·∫°o embedding cho chunk: {chunk[:100]}...")
                except Exception as e:
                    self.stdout.write(
                        self.style.WARNING(f"L·ªói khi t·∫°o embedding: {e}")
                    )
                    continue
            
            processed += 1
            progress = (processed / total_texts) * 100
            self.stdout.write(f"Ti·∫øn ƒë·ªô: {progress:.1f}% ({processed}/{total_texts})")
        
        # Ch·ªâ l∆∞u n·∫øu c√≥ d·ªØ li·ªáu
        if all_chunks and all_embeddings:
            # L∆∞u v√†o JSON
            data = {
                'chunks': all_chunks,
                'embeddings': all_embeddings,
                'metadata': {
                    'total_chunks': len(all_chunks),
                    'chunk_size': chunk_size,
                    'model': 'models/embedding-001',
                    'created_at': str(django.utils.timezone.now())
                }
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            self.stdout.write(
                self.style.SUCCESS(f"üéâ ƒê√£ l∆∞u {len(all_chunks)} chunks v√† embeddings v√†o {output_file}")
            )
            
            # Hi·ªÉn th·ªã th·ªëng k√™
            self.stdout.write("\n" + "="*50)
            self.stdout.write("üìä TH·ªêNG K√ä:")
            self.stdout.write(f"‚Ä¢ T·ªïng s·ªë vƒÉn b·∫£n g·ªëc: {total_texts}")
            self.stdout.write(f"‚Ä¢ T·ªïng s·ªë chunks: {len(all_chunks)}")
            self.stdout.write(f"‚Ä¢ K√≠ch th∆∞·ªõc chunk t·ªëi ƒëa: {chunk_size} t·ª´")
            self.stdout.write(f"‚Ä¢ Model embedding: models/embedding-001")
            self.stdout.write(f"‚Ä¢ File output: {output_file}")
            self.stdout.write("="*50)
            
        else:
            self.stdout.write(
                self.style.ERROR("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng")
            )